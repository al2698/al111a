# 爬山演算法

首先，我先詢問Chat 爬山演算法

```
Here is an outline of the steps you can follow to solve this problem using the hill climbing algorithm:
	1. Define the objective function and constraints as equations. For example, the objective function can be represented as: f(x1, x2, x3, x4, x5) = 7x1 + 8x2 + 2x3 + 9x4 + 6x5 And the constraints can be represented as: 5x1 + 7x2 + 9x3 + 2x4 + 1x5 ≤ 250 18x1 + 4x2 – 9x3 + 10x4 + 12x5 ≤ 285 4x1 + 7x2 + 3x3 + 8x4 + 5x5 ≤ 211 5x1 + 13x2 + 16x3 + 3x4 – 7x5 ≤ 315
	2. Choose a random initial point within the feasible region defined by the constraints.
	3. Calculate the value of the objective function at the current point.
	4. Generate a set of neighboring points and calculate the value of the objective function at each of these points.
	5. Move to the neighboring point that gives the highest value for the objective function.
	6. Repeat steps 3-5 until you reach a local maximum.
```

## C

```python
import random

# Define the objective function to maximize
def objective_function(x):
  return 7*x[0] + 8*x[1] + 2*x[2] + 9*x[3] + 6*x[4]

# Define the constraints
def constraints(x):
  return 5*x[0] + 7*x[1] + 9*x[2] + 2*x[3] + 1*x[4] <= 250 and \
         18*x[0] + 4*x[1] - 9*x[2] + 10*x[3] + 12*x[4] <= 285 and \
         4*x[0] + 7*x[1] + 3*x[2] + 8*x[3] + 5*x[4] <= 211 and \
         5*x[0] + 13*x[1] + 16*x[2] + 3*x[3] - 7*x[4] <= 315

# Initialize the variables
x = [random.randint(0, 10), random.randint(0, 10), random.randint(0, 10), random.randint(0, 10), random.randint(0, 10)]

# Set the step size
step_size = 5

# Set the maximum number of iterations
max_iterations = 10000

# Set the stopping criteria
tolerance = 1

# Initialize the loop variables
best_x = x
best_score = objective_function(best_x)
converged = False
iteration = 0

# Loop until convergence or maximum number of iterations is reached
while not converged and iteration < max_iterations:
  # Generate a random neighbor
  neighbor = [x[0] + random.randint(-step_size, step_size), x[1] + random.randint(-step_size, step_size), x[2] + random.randint(-step_size, step_size), x[3] + random.randint(-step_size, step_size), x[4] + random.randint(-step_size, step_size)]
  
  # Check if the neighbor is valid (satisfies the constraints)
  if constraints(neighbor):
    # Evaluate the neighbor
    score = objective_function(neighbor)
    
    # Update the best solution if necessary
    if score > best_score:
      best_x = neighbor
      best_score = score
  
  # Check for convergence
  if objective_function(x) - best_score > tolerance:
    converged = True
  
  # Update the loop variables
  x = best_x
  iteration += 1

# Print the optimized objective function value and the optimal solution
print('iteration', iteration)
print(best_score)
print(best_x)
```

ANS：

```
iteration 10000
257
[9, 10, 3, 6, 9] #答案不知道有沒有xi從零開始的限制
```

---

我問chat 如何達到改善爬山演算法：

```
There are several ways you can try to improve the chances of finding the global optimum of an optimization problem:

1. Use a different search algorithm: Different optimization algorithms have different strengths and weaknesses, and some are better suited to certain types of problems than others. It may be worth trying a different optimization algorithm to see if it gives a better result.

2. Use multiple runs of the same algorithm: Some optimization algorithms, such as the hill-climbing algorithm, are sensitive to the starting point of the search. To improve the chances of finding the global optimum, you can try running the algorithm multiple times with different starting points and then selecting the best solution found across all runs.

3. Use global optimization algorithms: Global optimization algorithms are designed to search the entire solution space and are more likely to find the global optimum. Examples of global optimization algorithms include simulated annealing and evolutionary algorithms.

4. Use deterministic methods: Deterministic optimization methods, such as the simplex method, are guaranteed to find the global optimum if it exists. However, these methods can be computationally expensive and may not be practical for large or complex optimization problems.

5. Use problem-specific techniques: Depending on the specific characteristics of the optimization problem, there may be problem-specific techniques or heuristics that can be used to improve the chances of finding the global optimum. For example, if the objective function is smooth and has a single global maximum, gradient descent or Newton's method may be effective.

It is worth noting that there is no guarantee that any optimization algorithm will find the global optimum, and it is always possible that the global optimum may not be found even after trying multiple approaches. However, by using a combination of these techniques, you can improve the chances of finding the global optimum and finding a good solution to the problem.
```

他提出使用 multi-armed bandit 算法 

---

## 方法二：[蒙特卡羅方法](https://wiki.mbalib.com/zh-tw/蒙特卡罗方法)變形

```python
import random

# Define the objective function to maximize
def objective_function(x):
  return 7*x[0] + 8*x[1] + 2*x[2] + 9*x[3] + 6*x[4]

# Define the constraints
def constraints(x):
  return 5*x[0] + 7*x[1] + 9*x[2] + 2*x[3] + 1*x[4] <= 250 and \
         18*x[0] + 4*x[1] - 9*x[2] + 10*x[3] + 12*x[4] <= 285 and \
         4*x[0] + 7*x[1] + 3*x[2] + 8*x[3] + 5*x[4] <= 211 and \
         5*x[0] + 13*x[1] + 16*x[2] + 3*x[3] - 7*x[4] <= 315

# Number of arms (possible solutions)
n = 10000

# Initialize the arms (possible solutions) with random values
arms = [[random.randint(0, 20) for _ in range(5)] for _ in range(n)]

# Initialize the number of times each arm has been evaluated
evaluations = [0 for _ in range(n)]

# Initialize the scores for each arm
scores = [0 for _ in range(n)]

# Initialize the current best arm
best_arm = 0

# Set the maximum number of iterations
max_iterations = 1000

# Loop until maximum number of iterations is reached
for i in range(max_iterations):
    # Choose an arm
    arm = random.randint(0, n-i)

    # Check if the arm is valid (satisfies the constraints)
    if constraints(arms[arm]):
        if evaluations[best_arm] == 0:
            best_arm = arm
        
        # Evaluate the arm
        score = objective_function(arms[arm])

        # Update the number of times the arm has been evaluated
        evaluations[arm] += 1

        # Update the scores for the arm
        scores[arm] += score

        # Calculate the average score for the arm
        avg_score = scores[arm] / evaluations[arm]

        # Update the current best arm if necessary
        if avg_score > scores[best_arm] / evaluations[best_arm]:
          best_arm = arm

# Print the optimal solution and its score
print(f'Optimal solution: {arms[best_arm]}')
print(f'best arm: {best_arm}')
print(f'Optimal score: {int(scores[best_arm]/evaluations[best_arm])}')
```

還值得注意的是，隨機搜索通常不被認為是一種非常有效的優化方法，尤其是對於具有大量變量或大搜索空間的問題。更有效的方法，例如基於梯度的優化或進化算法，可能更適合這些類型的問題。

---

參考 ChatGPT and 資工三 1110910542 徐伯元
